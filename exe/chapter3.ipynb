{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 演習\n",
    "- XORをpythonで実装し,正解を確認してください."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AND(x, y):\n",
    "    w_1 = 0.5\n",
    "    w_2 = 0.5\n",
    "    theta = 0.7\n",
    "    return 1 if  w_1 * x + w_2 * y >= theta else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0 1\n"
     ]
    }
   ],
   "source": [
    "print(AND(1, 0), AND(0, 1), AND(0, 0), AND(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OR(x, y):\n",
    "    w_1 = 0.5\n",
    "    w_2 = 0.5\n",
    "    theta = 0.3\n",
    "    return 1 if  w_1 * x + w_2 * y >= theta else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 0 1\n"
     ]
    }
   ],
   "source": [
    "print(OR(1, 0), OR(0, 1), OR(0, 0), OR(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NAND(x, y):\n",
    "    w_1 = - 0.5\n",
    "    w_2 = - 0.5\n",
    "    theta = - 0.7\n",
    "    return 1 if  w_1 * x + w_2 * y >= theta else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 1 0\n"
     ]
    }
   ],
   "source": [
    "print(NAND(1, 0), NAND(0, 1), NAND(0, 0), NAND(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XOR(x, y):\n",
    "    return AND(NAND(x, y), OR(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 0 0\n"
     ]
    }
   ],
   "source": [
    "print(XOR(1, 0), XOR(0, 1), XOR(0, 0), XOR(1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 演習\n",
    "以下を微分せよ.ただし、$ReLU$は$x=0$では劣微分を求めてください.\n",
    "- sigmoid :$\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n",
    "- ReLU: $ReLU(x) = \\max\\\\{x, 0\\\\}$\n",
    "\n",
    "### 回答\n",
    "- $(1-\\sigma(x))\\sigma(x)$\n",
    "- ReLU\n",
    "  - $1 (x > 0)$\n",
    "  - $0 (x < 0)$\n",
    "  - $[0, 1] x = 0$\n",
    "## 演習 2\n",
    "- 3層のNNのクラスを実装せよ\n",
    "  - 入力3次元\n",
    "  - 出力4次元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = nn.Linear(3, 5)\n",
    "        self.l2 = nn.Linear(5, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.l1(x)\n",
    "        y = self.l2(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7255,  0.2169,  0.0748, -0.4225], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "x = torch.arange(3).float()\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 演習\n",
    "実際に3層のNNに対して,誤差逆伝搬を使い,損失の微分を計算しよう.\n",
    "- 活性化関数は全て$sigmoid$\n",
    "- 入力(1,1), 出力(1,0)\n",
    "- 損失は二乗誤差\n",
    "$$\n",
    "W_1 = \\begin{pmatrix}\n",
    "2 & 3 \\\\\n",
    "1 & 4 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "$$\n",
    "W_2 = \\begin{pmatrix}\n",
    "3 & -1 \\\\\n",
    "0 & 4 \\\\\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = torch.tensor([[2., 3.], [1., 4.]], requires_grad=True)\n",
    "W2 = torch.tensor([[3., -1.], [0., 4.]], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1., 1.], requires_grad=True)\n",
    "y = torch.tensor([1., 0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.functional import F\n",
    "sigma = F.sigmoid\n",
    "mse = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 出力の計算\n",
    "o = F.linear(x, W1)\n",
    "o = sigma(o)\n",
    "o = F.linear(o, W2)\n",
    "o = sigma(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 損失\n",
    "loss = mse(o, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([4.7808e-05, 1.4670e-03]),)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.grad(loss, x, retain_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 演習\n",
    "- $W$は2×2の行列\n",
    "- $x,y$は2次元のベクトル\n",
    "- $loss = |y - Wx|^2$の時\n",
    "- $x,y, W$から$loss$までを計算する計算グラフを書きましょう\n",
    "- 計算グラフに従って微分の計算をしてみてください\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 回答\n",
    "- 手計算が想定回答ですが、実装するなら"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.tensor([[2., 3.], [1., 4.]], requires_grad=True)\n",
    "x = torch.tensor([1., 1.], requires_grad=True)\n",
    "y = torch.tensor([1., 0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = F.linear(x, W)\n",
    "loss = mse(z, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward object at 0x7f5f9980f9d0>\n",
      "squeezeは不要な次元を削除 ((<SqueezeBackward3 object at 0x7f5f9a54c290>, 0),)\n",
      "行列の掛け算 ((<MmBackward object at 0x7f5f9a634390>, 0),)\n",
      "((<UnsqueezeBackward0 object at 0x7f5f99af0390>, 0), (<TBackward object at 0x7f5f99af0e50>, 0))\n",
      "<UnsqueezeBackward0 object at 0x7f5f99af0390>\n",
      "accumulateは累積和 <AccumulateGrad object at 0x7f5f9af78ed0>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)\n",
    "print(\"squeezeは不要な次元を削除\", loss.grad_fn.next_functions, )\n",
    "\n",
    "print(\"行列の掛け算\", loss.grad_fn.next_functions[0][0].next_functions)\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0].next_functions)\n",
    "# Unsquessze 型の変換\n",
    "# T: 転置\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0]\\\n",
    ".next_functions[0][0])\n",
    "print(\"accumulateは累積和\", loss.grad_fn.next_functions[0][0].next_functions[0][0]\\\n",
    ".next_functions[0][0].next_functions[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.grad_fn.next_functions[0][0].next_functions[0][0]\\\n",
    ".next_functions[0][0].next_functions[0][0](torch.tensor([1, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((<MmBackward at 0x7f5f9a634390>, 0),)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.grad_fn.next_functions[0][0].next_functions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([13., 32.])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 4.],\n",
       "        [5., 5.]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 演習\n",
    "- Pytorchで(3層の)DNNを使い`iris`の分類を行ってください.\n",
    "- 損失関数はcross_entropyを使ってください.\n",
    "- optimzierをSGDで学習してください"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "iris = load_iris()\n",
    "\n",
    "# データの分割\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(iris.data, iris.target, test_size=0.2)\n",
    "\n",
    "class IrisDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        data = self.data[index]\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        if not self.transform is None:\n",
    "            data, label = self.transform(data, label)\n",
    "        \n",
    "        return data, label\n",
    "def iris_transform(data, label):\n",
    "    # inputはdoubleをfloatにしている.\n",
    "    return torch.tensor(data).float(), torch.tensor(label)\n",
    "\n",
    "iris = load_iris()\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(iris.data, iris.target, test_size=0.2)\n",
    "\n",
    "iris_train_dataset = IrisDataset(X_train, y_train, iris_transform)\n",
    "iris_valid_dataset = IrisDataset(X_valid, y_valid, iris_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size  = 30 # ミニバッチのデータの数\n",
    "train_loader = torch.utils.data.DataLoader(iris_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(iris_valid_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = nn.Linear(4, 3)\n",
    "        self.l2 = nn.Linear(3, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = F.relu(out)\n",
    "        y = self.l2(out)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "\n",
    "max_epoch = 100 #\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # 損失の定義\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01) #(確率的)勾配降下法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(data_loader): # 入力と正解\n",
    "         optimizer.zero_grad() # Weightの初期化\n",
    "         output = model(data) # 仮説で値代入\n",
    "         output.dtype\n",
    "         loss = criterion(output, target) # 損失\n",
    "         loss.backward() # 微分の計算\n",
    "         optimizer.step() # パラメータの更新\n",
    "         \n",
    "    if epoch % 10 == 0:\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            batch_idx, batch_idx * len(data), len(data_loader.dataset),\n",
    "            100. * batch_idx / len(data_loader), loss.item()))\n",
    "\n",
    "def valid_epoch(model, data_loader, epoch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(data_loader): # 入力と正解\n",
    "             optimizer.zero_grad() # Weightの初期化\n",
    "             output = model(data) # 仮説で値代入\n",
    "             output.dtype\n",
    "             loss = criterion(output, target) # 損失\n",
    "            \n",
    "        if epoch % 10 == 0:\n",
    "             print('Test Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                 batch_idx, batch_idx * len(data), len(data_loader.dataset),\n",
    "                 100. * batch_idx / len(data_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [90/120 (75%)]\tLoss: 0.166712\n",
      "Test Epoch: 0 [0/30 (0%)]\tLoss: 0.080890\n",
      "Train Epoch: 3 [90/120 (75%)]\tLoss: 0.176670\n",
      "Test Epoch: 0 [0/30 (0%)]\tLoss: 0.079282\n",
      "Train Epoch: 3 [90/120 (75%)]\tLoss: 0.068848\n",
      "Test Epoch: 0 [0/30 (0%)]\tLoss: 0.077608\n",
      "Train Epoch: 3 [90/120 (75%)]\tLoss: 0.044793\n",
      "Test Epoch: 0 [0/30 (0%)]\tLoss: 0.080907\n",
      "Train Epoch: 3 [90/120 (75%)]\tLoss: 0.022234\n",
      "Test Epoch: 0 [0/30 (0%)]\tLoss: 0.080694\n",
      "Train Epoch: 3 [90/120 (75%)]\tLoss: 0.032296\n",
      "Test Epoch: 0 [0/30 (0%)]\tLoss: 0.080627\n",
      "Train Epoch: 3 [90/120 (75%)]\tLoss: 0.039864\n",
      "Test Epoch: 0 [0/30 (0%)]\tLoss: 0.083666\n",
      "Train Epoch: 3 [90/120 (75%)]\tLoss: 0.050364\n",
      "Test Epoch: 0 [0/30 (0%)]\tLoss: 0.079047\n",
      "Train Epoch: 3 [90/120 (75%)]\tLoss: 0.040048\n",
      "Test Epoch: 0 [0/30 (0%)]\tLoss: 0.078542\n",
      "Train Epoch: 3 [90/120 (75%)]\tLoss: 0.056868\n",
      "Test Epoch: 0 [0/30 (0%)]\tLoss: 0.078962\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    train_epoch(model, train_loader, epoch)\n",
    "    valid_epoch(model, valid_loader, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 演習\n",
    "- Pytorchで(3層の)DNNを使い`iris`の分類を行ってください.\n",
    "- 損失関数はcross_entropyを使ってください.\n",
    "- optimzierをAdam/RMSPropでも学習してみてください"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 演習\n",
    "- PytorchでDNNを使いMNISTの分類を行ってください.\n",
    "ただし、以下は行ってください.\n",
    "- weight_decay\n",
    "- Dropout\n",
    "- BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    MNIST('./mnist', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        # 行列を定義\n",
    "        self.l1 = nn.Linear(input_size, input_size)\n",
    "        self.l2 = nn.Linear(input_size, output_size)\n",
    "        self.bm = nn.BatchNorm1d(input_size)\n",
    "        self.do = nn.Dropout(p=0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_size) # reshape\n",
    "        x = self.bm(x)\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = self.do(x)\n",
    "        return self.l2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(28*28, 10)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, weight_decay=0.01)\n",
    "optimizer.zero_grad()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer.step()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader):\n",
    "    model.train()\n",
    "    tot_loss = 0\n",
    "    for id, (data, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        tot_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if id % 300 == 0:\n",
    "            print(\"loss\", tot_loss)\n",
    "            pred = out.argmax(axis=1)\n",
    "            print(\"acc\", (pred == labels).sum().item() / labels.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 2.0840952396392822\n",
      "acc 0.28125\n",
      "loss 143.36962301284075\n",
      "acc 0.90625\n",
      "loss 240.52823129296303\n",
      "acc 0.9375\n",
      "loss 334.60667476058006\n",
      "acc 0.90625\n",
      "loss 428.2117058336735\n",
      "acc 0.96875\n",
      "loss 526.5103172361851\n",
      "acc 0.9375\n",
      "loss 629.6539689004421\n",
      "acc 0.96875\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "tot_loss = 0\n",
    "for id, (data, labels) in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = criterion(out, labels)\n",
    "    loss.backward()\n",
    "    tot_loss += loss.item()\n",
    "    optimizer.step()\n",
    "    if id % 300 == 0:\n",
    "        print(\"loss\", tot_loss)\n",
    "        pred = out.argmax(axis=1)\n",
    "        print(\"acc\", (pred == labels).sum().item() / labels.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

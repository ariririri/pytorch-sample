{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実装\n",
    "- クロスエントロピー\n",
    "- 仮説"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() # 損失の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4375)\n"
     ]
    }
   ],
   "source": [
    "preds = torch.tensor([[0.2, 0.8]]) # 二値分類\n",
    "labels = torch.tensor([1]) # 正解ラベル\n",
    "\n",
    "loss = criterion(preds, labels)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 演習\n",
    "- 3×3のモデル\n",
    "- クロスエントロピー"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(np.arange(12).reshape(4, 3)).float()\n",
    "Y = torch.tensor([1, 2, 0, 1])\n",
    "model = nn.Linear(3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2623, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(model(X), Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorchで\n",
    "- 勾配降下\n",
    "- ニュートン法\n",
    "\n",
    "で ルート2を求める\n",
    "\n",
    "- 勾配降下の場合\n",
    "  - $f(x) = x^3 - 6x$で計算する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F(x):\n",
    "    return x ** 3 -  6 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([0.01], requires_grad=True)\n",
    "y = F(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loop tensor([0.6100], grad_fn=<SubBackward0>)\n",
      "1 loop tensor([1.0984], grad_fn=<SubBackward0>)\n",
      "2 loop tensor([1.3364], grad_fn=<SubBackward0>)\n",
      "3 loop tensor([1.4006], grad_fn=<SubBackward0>)\n",
      "4 loop tensor([1.4121], grad_fn=<SubBackward0>)\n",
      "5 loop tensor([1.4139], grad_fn=<SubBackward0>)\n",
      "6 loop tensor([1.4142], grad_fn=<SubBackward0>)\n",
      "7 loop tensor([1.4142], grad_fn=<SubBackward0>)\n",
      "8 loop tensor([1.4142], grad_fn=<SubBackward0>)\n",
      "9 loop tensor([1.4142], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    y = F(x)\n",
    "    z = torch.autograd.grad(y, x)\n",
    "    x = x - learning_rate * z[0]\n",
    "    print(i, \"loop\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ニュートン法の場合\n",
    "  - $f(x) = x^2 - 2$で計算する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G(x):\n",
    "    return x ** 2 - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([0.1], requires_grad=True)\n",
    "y = G(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.9900], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 lootp tensor([10.0500], requires_grad=True)\n",
      "1 lootp tensor([5.1245], requires_grad=True)\n",
      "2 lootp tensor([2.7574], requires_grad=True)\n",
      "3 lootp tensor([1.7414], requires_grad=True)\n",
      "4 lootp tensor([1.4449], requires_grad=True)\n",
      "5 lootp tensor([1.4145], requires_grad=True)\n",
      "6 lootp tensor([1.4142], requires_grad=True)\n",
      "7 lootp tensor([1.4142], requires_grad=True)\n",
      "8 lootp tensor([1.4142], requires_grad=True)\n",
      "9 lootp tensor([1.4142], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    y = G(x)\n",
    "    y.backward()\n",
    "    # backwardで値を入れる時はdataに代入\n",
    "    # xに代入すると,backwardの対象でなくなる\n",
    "    x.data = x.data - y/ x.grad\n",
    "    print(i, \"lootp\", x)\n",
    "    x.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax回帰の実装\n",
    "- irisに対し,softmax回帰で実装する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(data_loader): # 入力と正解\n",
    "         optimizer.zero_grad() # Weightの初期化\n",
    "         output = model(data) # 仮説で値代入\n",
    "         loss = criterion(output, target) # 損失\n",
    "         loss.backward() # 微分の計算\n",
    "         optimizer.step() # パラメータの更新\n",
    "         if batch_idx % 10 == 0:\n",
    "             print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                 batch_idx, batch_idx * len(data), len(data_loader.dataset),\n",
    "                 100. * batch_idx / len(data_loader), loss.item()))\n",
    "\n",
    "def valid_epoch(model, data_loader):\n",
    "    model.train()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(data_loader): # 入力と正解\n",
    "             optimizer.zero_grad() # Weightの初期化\n",
    "             output = model(data) # 仮説で値代入\n",
    "             output.dtype\n",
    "             loss = criterion(output, target) # 損失\n",
    "             # 本来は全体でロスを数えて荷重平均を取る,accuracyを計算する\n",
    "\n",
    "             if batch_idx % 10 == 0:\n",
    "                 print('Test Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                     batch_idx, batch_idx * len(data), len(data_loader.dataset),\n",
    "                     100. * batch_idx / len(data_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(iris.data, iris.target, test_size=0.2)\n",
    "\n",
    "X_train = torch.tensor(X_train).float()\n",
    "y_train = torch.tensor(y_train)\n",
    "X_valid = torch.tensor(X_valid).float()\n",
    "y_valid = torch.tensor(y_valid)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "valid_dataset = torch.utils.data.TensorDataset(X_valid, y_valid)\n",
    "\n",
    "model = nn.Linear(4, 3)\n",
    "\n",
    "batch_size  = 120 # ミニバッチのデータの数\n",
    "max_epoch = 100 #\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                   batch_size=batch_size, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                   batch_size=batch_size, shuffle=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # 損失の定義\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01) #(確率的)勾配降下法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 1.360266\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 1.268214\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 1.268214\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 1.194083\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 1.194083\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 1.134569\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 1.134568\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 1.087408\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 1.087408\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 1.050642\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 1.050642\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 1.022376\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 1.022377\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 1.000799\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 1.000799\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.984276\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.984276\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.971439\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.971439\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.961214\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.961214\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.952797\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.952797\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.945619\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.945618\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.939281\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.939281\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.933519\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.933519\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.928153\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.928153\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.923068\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.923068\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.918188\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.918188\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.913463\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.913463\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.908861\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.908861\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.904362\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.904362\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.899950\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.899950\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.895617\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.895618\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.891357\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.891357\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.887165\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.887165\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.883036\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.883036\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.878970\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.878970\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.874963\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.874963\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.871015\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.871015\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.867123\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.867123\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.863287\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.863287\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.859505\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.859505\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.855776\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.855776\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.852099\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.852099\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.848473\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.848473\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.844897\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.844898\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.841371\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.841371\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.837893\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.837894\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.834463\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.834463\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.831079\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.831079\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.827741\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.827741\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.824448\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.824448\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.821198\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.821199\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.817993\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.817993\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.814830\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.814829\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.811708\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.811708\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.808628\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.808628\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.805588\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.805588\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.802588\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.802588\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.799627\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.799626\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.796704\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.796703\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.793818\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.793818\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.790969\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.790970\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.788157\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.788157\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.785380\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.785380\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.782639\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.782639\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.779931\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.779931\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.777258\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.777258\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.774618\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.774617\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.772010\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.772010\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.769434\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.769434\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.766890\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.766890\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.764376\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.764376\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.761893\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.761893\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.759440\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.759440\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.757017\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.757017\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.754622\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.754622\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.752255\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.752255\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.749917\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.749917\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.747606\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.747606\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.745322\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.745322\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.743065\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.743064\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.740833\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.740832\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.738627\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.738627\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.736446\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.736446\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.734290\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.734290\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.732158\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.732158\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.730050\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.730050\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.727966\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.727966\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.725905\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.725905\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.723867\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.723867\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.721851\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.721851\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.719858\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.719858\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.717886\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.717886\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.715935\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.715935\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.714005\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.714005\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.712096\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.712096\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.710207\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.710207\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.708338\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.708339\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.706489\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.706489\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.704660\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.704660\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.702849\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.702849\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.701057\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.701057\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.699284\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.699284\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.697529\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.697528\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.695791\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.695791\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.694071\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.694071\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.692369\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.692369\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.690683\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.690683\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.689015\n",
      "Train Epoch: 0 [0/120 (0%)]\tLoss: 0.689015\n",
      "Test Epoch: 0 [0/120 (0%)]\tLoss: 0.687363\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(max_epoch):\n",
    "    train_epoch(model, train_loader)\n",
    "    valid_epoch(model, valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaptivePooling\n",
    "毎回挙動が怪しくなるので"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.AdaptiveMaxPool2d(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(np.arange(24).reshape(1, 3, 8).astype(float))\n",
    "output = m(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 8])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11., 12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19., 20., 21., 22., 23.]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0000,  3.5000,  6.0000],\n",
       "         [ 9.0000, 11.5000, 14.0000],\n",
       "         [17.0000, 19.5000, 22.0000]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

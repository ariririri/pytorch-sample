{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実装\n",
    "- クロスエントロピー\n",
    "- 仮説"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() # 損失の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4375)\n"
     ]
    }
   ],
   "source": [
    "preds = torch.tensor([[0.2, 0.8]]) # 二値分類\n",
    "labels = torch.tensor([1]) # 正解ラベル\n",
    "\n",
    "loss = criterion(preds, labels)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 演習\n",
    "- 3×3のモデル\n",
    "- クロスエントロピー"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(np.arange(12).reshape(4, 3)).float()\n",
    "Y = torch.tensor([1, 2, 0, 1])\n",
    "model = nn.Linear(3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.0542, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(model(X), Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorchで\n",
    "- 勾配降下\n",
    "- ニュートン法\n",
    "\n",
    "で ルート2を求める\n",
    "\n",
    "- 勾配降下の場合\n",
    "  - $f(x) = x^3 - 6x$で計算する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F(x):\n",
    "    return x ** 3 -  6 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([0.01], requires_grad=True)\n",
    "y = F(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loop tensor([0.6100], grad_fn=<SubBackward0>)\n",
      "1 loop tensor([1.0984], grad_fn=<SubBackward0>)\n",
      "2 loop tensor([1.3364], grad_fn=<SubBackward0>)\n",
      "3 loop tensor([1.4006], grad_fn=<SubBackward0>)\n",
      "4 loop tensor([1.4121], grad_fn=<SubBackward0>)\n",
      "5 loop tensor([1.4139], grad_fn=<SubBackward0>)\n",
      "6 loop tensor([1.4142], grad_fn=<SubBackward0>)\n",
      "7 loop tensor([1.4142], grad_fn=<SubBackward0>)\n",
      "8 loop tensor([1.4142], grad_fn=<SubBackward0>)\n",
      "9 loop tensor([1.4142], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    y = F(x)\n",
    "    z = torch.autograd.grad(y, x)\n",
    "    x = x - learning_rate * z[0]\n",
    "    print(i, \"loop\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ニュートン法の場合\n",
    "  - $f(x) = x^2 - 2$で計算する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G(x):\n",
    "    return x ** 2 - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([0.1], requires_grad=True)\n",
    "y = G(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.9900], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 lootp tensor([10.0500], requires_grad=True)\n",
      "1 lootp tensor([5.1245], requires_grad=True)\n",
      "2 lootp tensor([2.7574], requires_grad=True)\n",
      "3 lootp tensor([1.7414], requires_grad=True)\n",
      "4 lootp tensor([1.4449], requires_grad=True)\n",
      "5 lootp tensor([1.4145], requires_grad=True)\n",
      "6 lootp tensor([1.4142], requires_grad=True)\n",
      "7 lootp tensor([1.4142], requires_grad=True)\n",
      "8 lootp tensor([1.4142], requires_grad=True)\n",
      "9 lootp tensor([1.4142], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    y = G(x)\n",
    "    y.backward()\n",
    "    # backwardで値を入れる時はdataに代入\n",
    "    # xに代入すると,backwardの対象でなくなる\n",
    "    x.data = x.data - y/ x.grad\n",
    "    print(i, \"lootp\", x)\n",
    "    x.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorchで実装するべきこと\n",
    "0. データ準備\n",
    "1. モデル設計\n",
    "    - パラメータ/仮説\n",
    "    - 損失関数\n",
    "2. 学習\n",
    "    - 勾配を計算\n",
    "    - 勾配降下法によってパラメータ更新\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename'])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "print(iris[\"feature_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "# データの分割\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(iris.data, iris.target, test_size=0.2)\n",
    "\n",
    "# Pytorchのデータ型への変更\n",
    "X_train = torch.tensor(X_train).float()\n",
    "y_train = torch.tensor(y_train)\n",
    "X_valid = torch.tensor(X_valid).float()\n",
    "y_valid = torch.tensor(y_valid)\n",
    "\n",
    "# Datasetの作成\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "valid_dataset = torch.utils.data.TensorDataset(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasetとは\n",
    "- データの集まり\n",
    "- サイズ\n",
    "- 特定のインデクスでデータを取れる\n",
    "- 前処理を上から加えられる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasetの抽象クラス\n",
    "- 実際こういう定義のはず"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return ConcatDataset([self, other])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrisDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        data = self.data[index]\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        if not self.transform is None:\n",
    "            data, label = self.transform(data, label)\n",
    "        \n",
    "        return data, label\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iris_transform(data, label):\n",
    "    return torch.tensor(data).float(), torch.tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(iris.data, iris.target, test_size=0.2)\n",
    "\n",
    "iris_train_dataset = IrisDataset(X_train, y_train, iris_transform)\n",
    "iris_valid_dataset = IrisDataset(X_valid, y_valid, iris_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn=None, pin_memory=False, drop_last=False, timeout=0, worker_init_fn=None, multiprocessing_context=None)\n",
    "```\n",
    "\n",
    "- dataset: datasetならOK(`__len__`, `__add__`)が必要\n",
    "- `batch_size`: 一つのデータ数\n",
    "- `shuffle, sampler, batch_sampler`: データの選び方\n",
    "- `num_workers`: プロセスの数\n",
    "- `collate_fn`: `DataLoeader`での後処理\n",
    "- `pin_memory`: `CUDA`用の設定、GPUのメモリでページングしない設定\n",
    "- `drop_last`: 最後のデータを使うか\n",
    "- `timeout, worker_init_fn, multiprocessing_context`: 今回は省略(最初は使わないはず)\n",
    "\n",
    "基本的にはデータを連結してiteratorとしてくれるもの"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([6.3000, 3.3000, 6.0000, 2.5000]), tensor(2))\n"
     ]
    }
   ],
   "source": [
    "for x in iris_train_dataset:\n",
    "    print(x)\n",
    "    break\n",
    "    # batchにはなっていない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size  = 24 # ミニバッチのデータの数\n",
    "iris_train_dataloader = torch.utils.data.DataLoader(iris_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "iris_valid_dataloader = torch.utils.data.DataLoader(iris_valid_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自作datasetを使わない実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(iris.data, iris.target, test_size=0.2)\n",
    "\n",
    "X_train = torch.tensor(X_train).float()\n",
    "y_train = torch.tensor(y_train)\n",
    "X_valid = torch.tensor(X_valid).float()\n",
    "y_valid = torch.tensor(y_valid)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "valid_dataset = torch.utils.data.TensorDataset(X_valid, y_valid)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                   batch_size=batch_size, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                   batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax回帰の実装\n",
    "- irisに対し,softmax回帰で実装する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(4, 3)\n",
    "\n",
    "batch_size  = 120 # ミニバッチのデータの数\n",
    "max_epoch = 100 #\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # 損失の定義\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01) #(確率的)勾配降下法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習のサンプル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for data, target in train_loader: # 入力と正解\n",
    "    optimizer.zero_grad() # Weightの初期化\n",
    "    output = model(data) # 仮説で値代入\n",
    "    loss = criterion(output, target) # 損失\n",
    "    loss.backward() # 微分の計算\n",
    "    optimizer.step() # パラメータの更新\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-35fd1bc23bcb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/pytorch-sample-IYnN251Q-py3.7/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/pytorch-sample-IYnN251Q-py3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1621\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'precision'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m                                                  \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1623\u001b[0;31m                                                  zero_division=zero_division)\n\u001b[0m\u001b[1;32m   1624\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/pytorch-sample-IYnN251Q-py3.7/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/pytorch-sample-IYnN251Q-py3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"beta should be >=0 in the F-beta score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m     labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n\u001b[0;32m-> 1434\u001b[0;31m                                     pos_label)\n\u001b[0m\u001b[1;32m   1435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1436\u001b[0m     \u001b[0;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/pytorch-sample-IYnN251Q-py3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1263\u001b[0m             raise ValueError(\"Target is %s but average='binary'. Please \"\n\u001b[1;32m   1264\u001b[0m                              \u001b[0;34m\"choose another average setting, one of %r.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1265\u001b[0;31m                              % (y_type, average_options))\n\u001b[0m\u001b[1;32m   1266\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mpos_label\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m         warnings.warn(\"Note that pos_label (set to %r) is ignored when \"\n",
      "\u001b[0;31mValueError\u001b[0m: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted']."
     ]
    }
   ],
   "source": [
    "            pred = output.argmax(axis=1).cpu().detach().numpy()\n",
    "            ans = target.cpu().numpy()\n",
    "            print(\"accuracy\", precision_score(ans, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.01\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルのweight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.01\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Parameter containing:\n",
      "tensor([[ 0.0195, -0.2274,  0.0058,  0.2803],\n",
      "        [-0.2773,  0.2211,  0.0552, -0.2181],\n",
      "        [-0.4248, -0.3287,  0.0324,  0.1256]], requires_grad=True)\n",
      "tensor([[ 3.2955,  1.4236,  2.9334,  1.1214],\n",
      "        [-0.7392, -0.2208, -0.6876, -0.2450],\n",
      "        [-2.5563, -1.2028, -2.2457, -0.8764]])\n"
     ]
    }
   ],
   "source": [
    "print(model.weight)\n",
    "print(model.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0195, -0.2274,  0.0058,  0.2803],\n",
      "        [-0.2773,  0.2211,  0.0552, -0.2181],\n",
      "        [-0.4248, -0.3287,  0.0324,  0.1256]], requires_grad=True)\n",
      "tensor([[ 3.2955,  1.4236,  2.9334,  1.1214],\n",
      "        [-0.7392, -0.2208, -0.6876, -0.2450],\n",
      "        [-2.5563, -1.2028, -2.2457, -0.8764]])\n"
     ]
    }
   ],
   "source": [
    "# after\n",
    "print(model.weight)\n",
    "print(model.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# モデルの勾配が消える\n",
    "model.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(data) # 仮説で値代入\n",
    "loss = criterion(output, target) # 損失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward() # 微分の計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.4398,  1.0046,  4.0171,  1.5467],\n",
       "        [-3.3215, -1.5426, -2.4041, -0.7414],\n",
       "        [-0.1183,  0.5379, -1.6130, -0.8053]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0349,  0.2804,  0.2858,  0.1865],\n",
       "        [-0.4398,  0.2456,  0.1448,  0.4426],\n",
       "        [ 0.4975,  0.1777, -0.3319, -0.3443]], requires_grad=True)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step() # パラメータの更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0339,  0.2603,  0.2054,  0.1555],\n",
       "        [-0.3733,  0.2764,  0.1929,  0.4575],\n",
       "        [ 0.4999,  0.1670, -0.2996, -0.3282]], requires_grad=True)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習・評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(data_loader): # 入力と正解\n",
    "         optimizer.zero_grad() # Weightの初期化\n",
    "         output = model(data) # 仮説で値代入\n",
    "         loss = criterion(output, target) # 損失\n",
    "         loss.backward() # 微分の計算\n",
    "         optimizer.step() # パラメータの更新\n",
    "    if epoch % 10 == 0:\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        batch_idx, batch_idx * len(data), len(data_loader.dataset),\n",
    "        100. * batch_idx / len(data_loader), loss.item()))\n",
    "\n",
    "def valid_epoch(model, data_loader, epoch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(data_loader): # 入力と正解\n",
    "             optimizer.zero_grad() # Weightの初期化\n",
    "             output = model(data) # 仮説で値代入\n",
    "             output.dtype\n",
    "             loss = criterion(output, target) # 損失\n",
    "             # 本来は全体でロスを数えて荷重平均を取る,accuracyを計算する\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print('Test Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                batch_idx, batch_idx * len(data), len(data_loader.dataset),\n",
    "                100. * batch_idx / len(data_loader), loss.item()))\n",
    "                \n",
    "            pred = output.argmax(axis=1).cpu().detach().numpy()\n",
    "            ans = target.cpu().numpy()\n",
    "            print(\"accuracy\", accuracy_score(ans, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 1.281663\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 1.059948\n",
      "accuracy 0.6666666666666666\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.912927\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.981117\n",
      "accuracy 0.16666666666666666\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.719185\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.715691\n",
      "accuracy 0.6666666666666666\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.659294\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.702804\n",
      "accuracy 0.8333333333333334\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.534567\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.784845\n",
      "accuracy 0.5\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.473775\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.671786\n",
      "accuracy 0.8333333333333334\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.397212\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.518721\n",
      "accuracy 0.6666666666666666\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.494810\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.478156\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.465431\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.538268\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.443261\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.285308\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.435657\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.471882\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.479526\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.541972\n",
      "accuracy 0.8333333333333334\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.410416\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.552762\n",
      "accuracy 0.6666666666666666\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.423941\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.373768\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.382118\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.526338\n",
      "accuracy 0.8333333333333334\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.363644\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.368891\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.360482\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.468830\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.376950\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.498310\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.353232\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.509702\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.375524\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.393608\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.328158\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.558371\n",
      "accuracy 0.8333333333333334\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.322395\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.192108\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.376774\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.298923\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.462522\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.350730\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.316512\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.278440\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.371197\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.504812\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.290512\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.325162\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.288641\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.247102\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.334092\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.333787\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.319357\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.272412\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.340100\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.274168\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.265591\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.326868\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.306015\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.318700\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.262053\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.240069\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.287683\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.247792\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.300316\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.208279\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.312325\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.437704\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.275847\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.216939\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.235380\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.275541\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.237554\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.257144\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.348550\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.293935\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.182318\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.241480\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.285142\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.266125\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.171555\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.276638\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.218130\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.380620\n",
      "accuracy 0.8333333333333334\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.187094\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.290818\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.232030\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.231540\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.230407\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.308363\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.245240\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.317499\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.235224\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.207516\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.258209\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.125978\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.243840\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.211970\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.253222\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.253875\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.217108\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.199900\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.168799\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.326798\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.209733\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.269909\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.231406\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.122711\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.205439\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.115535\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.272336\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.124083\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.262925\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.265139\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.230273\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.285795\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.172567\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.210476\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.216079\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.201267\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.216180\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.242988\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.288541\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.252142\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.166498\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.146430\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.169339\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.262558\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.193055\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.219196\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.220036\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.325495\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.229830\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.109848\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.196472\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.128126\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.235910\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.100981\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.206076\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.245292\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.233640\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.181047\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.196675\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.282021\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.194401\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.232555\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.187605\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.216796\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.208057\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.138575\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.178908\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.262411\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.199155\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.265963\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.281176\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.147564\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.175214\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.189498\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.221431\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.112169\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.209214\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.190237\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.167399\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.252976\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.212149\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.222505\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.200411\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.192764\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.179415\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.156803\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.164368\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.154664\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.116940\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.212647\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.212689\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.256231\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.179131\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.108365\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.179895\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.222595\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.214318\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.350128\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.165124\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.122647\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.179435\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.123256\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.199096\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.227858\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.157413\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.171581\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.174424\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.165007\n",
      "accuracy 1.0\n",
      "Train Epoch: 4 [96/120 (80%)]\tLoss: 0.131503\n",
      "Test Epoch: 1 [6/30 (50%)]\tLoss: 0.204996\n",
      "accuracy 1.0\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(max_epoch * 10):\n",
    "    train_epoch(model, iris_train_dataloader, epoch)\n",
    "    valid_epoch(model, iris_valid_dataloader, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 全然関係ないもの"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaptivePooling\n",
    "毎回挙動が怪しくなるので確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.AdaptiveMaxPool2d(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(np.arange(24).reshape(1, 3, 8).astype(float))\n",
    "output = m(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 8])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11., 12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19., 20., 21., 22., 23.]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0000,  3.5000,  6.0000],\n",
       "         [ 9.0000, 11.5000, 14.0000],\n",
       "         [17.0000, 19.5000, 22.0000]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.init as init\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## バージョンの確認\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch関係なく大前提として\n",
    "\n",
    "- 最近のOSSはドキュメント、ソースコードともにある程度読める\n",
    "- 何をしいていかわからない、うまく動かない場合は\n",
    "  - `エラーメッセージ`を見つつ、ドキュメントで仕様を確認し、ソースコードを含めエラーがないか確認すれば解決します。\n",
    "  - もちろん、聞いてくれるのは大歓迎なのですが、エンジニアリング力をあげるという意味では上のことができるようになった方がよいです。\n",
    "\n",
    "- 見るべきもの\n",
    "  - Pytorchのドキュメント: https://pytorch.org/docs/stable/index.html\n",
    "  - Pythonのドキュメント: https://docs.python.org/ja/3/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NNを楽に書くために必要なもの\n",
    "- Pythonに対する理解\n",
    "- Pytorchに対する理解\n",
    "- 機械学習に対する理解\n",
    "\n",
    "## NNライブラリを理解する上で難しいもの\n",
    "- 機械学習側もある程度理解しないとイメージしづらい\n",
    "  - 何をしているか\n",
    "  - なぜそそうているか"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 今回の紹介内容\n",
    "- Pytorchのnn等の中身について\n",
    "\n",
    "__注意事項__\n",
    "\n",
    "- どこが足りないか(Python?Pytoch?機械学習)を見つつ理解を楽しんでくれれば\n",
    "- Pytorchの内部に近い挙動を見ていくつもりなので、かなり難しいです。\n",
    "- 触りつつ、エラーメッセージみつつ、動作させてみて、理解を深めましょう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorchのモジュール構成(一部抜粋)\n",
    "```\n",
    ".\n",
    "├── autograd #自動微分系のもの\n",
    "├── backends #backend cuda/cudnn/mki etc\n",
    "├── contrib # 現在はtensorboardしかなかった?\n",
    "├── cuda # cudaはここも\n",
    "├── distributed # 分散コンピューティング用\n",
    "├── distributions # 確率分布\n",
    "├── for_onnx #onnx用モジュール ライブラリに依存しないNNのformat こっちは現状ほぼ中身がない\n",
    "├── jit # jit compile用\n",
    "├── lib # C++用\n",
    "├── multiprocessing # マルチプロセス用\n",
    "├── nn # nnのモジュール\n",
    "├── onnx # onnnx用モジュール\n",
    "├── optim # 最適化のモジュール\n",
    "├── quantization # 量子化\n",
    "├── sparse # スパース行列用\n",
    "├── testing # テスト用の準備\n",
    "├── utils # 便利ツール data, tensorboard等\n",
    "```\n",
    "\n",
    "大きく分けて2種類\n",
    "- 高速化・共通化・拡大化のためにあるもの(今回は触れない. CSの知識が必要)\n",
    "- NNの計算にはあるとよいもの(今回話すもの)\n",
    "  - Pythonレベルで知っておいてほしいもの"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorchで知っておくとよいもの\n",
    "- データの処理\n",
    "  - `torch.utils.data`: 前回話した\n",
    "- パラメータ更新の機能\n",
    "  - `optim`: 次回話すかも\n",
    "- ネットワークを保持するクラス(今日の話)\n",
    "  - nn.Parameters\n",
    "  - nn.Module\n",
    "  - nn.Linear\n",
    "  - nn.ModuleList\n",
    "  - nn.Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本的なPytorchのネットワークの作り方\n",
    "- 基本的な要素(行列の掛け算等)はPytorch側で用意\n",
    "  - ネットワークの一層、活性化関数\n",
    "  - 非常によく使われるNN(ResNet,LSTM)\n",
    "- それを組み合わせて新しネットワークのクラスを作成\n",
    "- ネットワワークのインスタンス `net`に対しては以下の操作ができる\n",
    "  - forward演算: 行列、活性化関数などをかける。 \n",
    "    - `net(x)`\n",
    "  - backward演算: 微分の計算\n",
    "    - `loss.backward()`\n",
    "- プログラムを書く時は、以下さえできればよい\n",
    "  - ネットワークの定義\n",
    "  - forward演算の結果\n",
    "  - 損失の計算\n",
    "\n",
    "```python\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, hoge, fuga, ....):\n",
    "        super()__init__\n",
    "        self.network1 = hoge\n",
    "        self.network2 = fuga\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.network1(x)\n",
    "        x = self.network2(x)\n",
    "        ...\n",
    "        retrun self.network_last(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ネットワークを実現するために\n",
    "1. パラメータとそれ以外の区別\n",
    "  - `nn.Parametes`\n",
    "2. モジュールを再帰的な構造にしている\n",
    "  - `nn.Module`\n",
    "3. (裏で実際に計算で利用した計算グラフの取得)\n",
    "  - 今回は紹介しない\n",
    "\n",
    "## 楽な計算をするために\n",
    "- 複数データ、1データでも同じ結果になるようにする"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 具体的なNNを見ていくためにPythonの基本的な道具について解説する\n",
    "\n",
    "- Pythonのクラスの初期化\n",
    "- OrderDict\n",
    "- generator\n",
    "- lambda 式\n",
    "\n",
    "\n",
    "### Pythonのクラス定義における `__init__`\n",
    "- クラスの初期化の時に実行されるもの\n",
    "- 以下に例を記述\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class initialized\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'fuga'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class InitSample():\n",
    "    def __init__(self, hoge):\n",
    "        print(\"class initialized\")\n",
    "        self.hoge = hoge\n",
    "\n",
    "init_sample = InitSample(\"fuga\")\n",
    "init_sample.hoge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Pythonでクラスのインスタンスを作成する場合(参考)\n",
    "1. `__new__` : クラス全体を見ながらインスタンス(`self`)作成\n",
    "2. `__init__`: インスタンス(`self`)の情報を初期化する\n",
    "\n",
    "あくまでそういう方針で作っているだけなので`__new__`で全部作れてしまう。\n",
    "ただ、ベースがそういう考え方であることは理解しておいてください\n",
    "\n",
    "## `__setattr__` (Pythonのメソッド)\n",
    "  `object.__setattr__(self, name, value)`\n",
    "  属性の代入が試みられた際に呼び出される\n",
    "  通常の代入の過程 (すなわち、インスタンス辞書への値の代入) の代わりに呼び出される。name は属性名で、value はその属性に代入する値です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name x\n",
      "value 0\n"
     ]
    }
   ],
   "source": [
    "# setattrのサンプル\n",
    "class SetAttrSample():\n",
    "    def __setattr__(self, name, value):\n",
    "        print(\"name\", name)\n",
    "        print(\"value\", value)\n",
    "        \n",
    "sample = SetAttrSample()\n",
    "sample.x = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SetAttrSample' object has no attribute 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-183-bc26201bfd62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;31m# 値が定義されてないのでエラーになる\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'SetAttrSample' object has no attribute 'x'"
     ]
    }
   ],
   "source": [
    "sample.x # 値が定義されてないのでエラーになる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name x\n",
      "value 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setattrのサンプル\n",
    "class SetAttrSample2():\n",
    "    def __setattr__(self, name, value):\n",
    "        print(\"name\", name)\n",
    "        print(\"value\", value)\n",
    "        object.__setattr__(self, name, value)\n",
    "        # self.__setattr__(self, name, value)\n",
    "        # self[name] = valeu だと無限ループになることに注意\n",
    "\n",
    "sample2 = SetAttrSample2()\n",
    "sample2.x = \"0\"\n",
    "sample2.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 上で出てきたobjectとは\n",
    "object は全てのクラスの基底クラス\n",
    "Pythonのクラスの全てのインスタンスに共通のメソッド群を持つ\n",
    "\n",
    "- 今回の場合:\n",
    "  - 親クラスの`__setattr__`メソッドを実行したことに相当\n",
    "\n",
    "### クラスの実行`__call__`:\n",
    "- `__call__`: が定義されていた場合\n",
    "  - net(x)のようだ動作ができる\n",
    "- 例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "class A:\n",
    "    def __call__(self, x):\n",
    "        print(x)\n",
    "a = A()\n",
    "a(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OrderDict(Pythonのモジュール)\n",
    "- 順序を保つDict\n",
    "- 順序を保つのでforループで同じ順番でkeyが得られる\n",
    "- collectionsモジュールの一つ\n",
    "- 最近のpythonではdictでも順序を保持している"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key k1\n",
      "val 1\n",
      "key k2\n",
      "val 2\n",
      "key k3\n",
      "val 3\n"
     ]
    }
   ],
   "source": [
    "# 例\n",
    "from collections import OrderedDict\n",
    "od = OrderedDict()\n",
    "\n",
    "od['k1'] = 1\n",
    "od['k2'] = 2\n",
    "od['k3'] = 3\n",
    "\n",
    "for key, val in od.items():\n",
    "    print(\"key\", key)\n",
    "    print(\"val\", val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([od])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generator\n",
    "- イテレータの一種で、要素を一つ取得する時に処理を加えて要素を渡すもの\n",
    "- Pythonではyield文を使った実装が多い\n",
    "- yieldはその値を取得して途中の結果が得られるもの"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'generator'>\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "def my_generator(x):\n",
    "    yield x+1\n",
    "    yield x+2\n",
    "    yield x+3\n",
    "\n",
    "gen = my_generator(3)\n",
    "print(type(gen))\n",
    "\n",
    "for g in gen:\n",
    "    print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plus_one_generator(x):\n",
    "    for _x in x:\n",
    "        yield _x + 1\n",
    "\n",
    "def plus_one_list(x):\n",
    "    return [_x + 1 for _x in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.18 µs ± 10.8 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit plus_one_generator(list(range(100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.95 µs ± 33.4 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit plus_one_list(list(range(100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lambda式\n",
    "- 無名関数ともいう\n",
    "- 書き方: `lambda  引数 :演算`\n",
    "- 演算結果が得られる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = lambda x: x ** 2\n",
    "f(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter(torch.Tensor):\n",
    "    r\"\"\"A kind of Tensor that is to be considered a module parameter.\n",
    "    Parameters are :class:`~torch.Tensor` subclasses, that have a\n",
    "    very special property when used with :class:`Module` s - when they're\n",
    "    assigned as Module attributes they are automatically added to the list of\n",
    "    its parameters, and will appear e.g. in :meth:`~Module.parameters` iterator.\n",
    "    Assigning a Tensor doesn't have such effect. This is because one might\n",
    "    want to cache some temporary state, like last hidden state of the RNN, in\n",
    "    the model. If there was no such class as :class:`Parameter`, these\n",
    "    temporaries would get registered too.\n",
    "    Arguments:\n",
    "        data (Tensor): parameter tensor.\n",
    "        requires_grad (bool, optional): if the parameter requires gradient. See\n",
    "            :ref:`excluding-subgraphs` for more details. Default: `True`\n",
    "    \"\"\"\n",
    "    # インスタンスを作成するメソッド\n",
    "    def __new__(cls, data=None, requires_grad=True):\n",
    "        if data is None:\n",
    "            data = torch.Tensor()\n",
    "        return torch.Tensor._make_subclass(cls, data, requires_grad)\n",
    "\n",
    "    # deepcopy実行時の挙動の指定\n",
    "    def __deepcopy__(self, memo):\n",
    "        if id(self) in memo:\n",
    "            return memo[id(self)]\n",
    "        else:\n",
    "            result = type(self)(self.data.clone(memory_format=torch.preserve_format), self.requires_grad)\n",
    "            memo[id(self)] = result\n",
    "            return result\n",
    "    \n",
    "    # repr()実行時にパラメータであることを明記\n",
    "    def __repr__(self):\n",
    "        return 'Parameter containing:\\n' + super(Parameter, self).__repr__()\n",
    "    \n",
    "    # pickleにする時に必要, hookを初期化している\n",
    "    def __reduce_ex__(self, proto):\n",
    "        # See Note [Don't serialize hooks]\n",
    "        return (\n",
    "            torch._utils._rebuild_parameter,\n",
    "            (self.data, self.requires_grad, OrderedDict())\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameterのまとめ\n",
    "- Parameter固有のメソッド類は少ない\n",
    "- ただし、Parameterとわかるようにrepr等が新しく設定されている\n",
    "- 使われ方はNNのパラメータかどうか、パラメータとパラメータ以外のtensorを区別するために別クラスを定義した"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(SimpleLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return F.linear(x, self.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7.3787e+19, 1.5849e+29, 0.0000e+00], grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = SimpleLinear(3, 3)\n",
    "l(torch.tensor([1. , 2., 3.]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Module\n",
    "- pytorchのNNを実行(forward)/微分(backward)する親玉のクラス\n",
    "- https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/module.py\n",
    "\n",
    "コメント\n",
    "```\n",
    "Base class for all neural network modules.\n",
    "Your models should also subclass this class.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Moduleのソースコード\n",
    "- それなりに大きいので、一部基本的なものだけみる\n",
    "- メソッド単位で分割する\n",
    "- 実際に何が起きてるかは実験しつつ確認する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `__init__` の処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Moduleの`__init__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class Module:\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
    "        \"\"\"\n",
    "        torch._C._log_api_usage_once(\"python.nn_module\")\n",
    "\n",
    "        self.training = True\n",
    "        # self._non_persistent_buffers_set = set() 最新のgithub上には存在()\n",
    "        self._parameters = OrderedDict()\n",
    "        self._buffers = OrderedDict()\n",
    "        self._backward_hooks = OrderedDict()\n",
    "        self._forward_hooks = OrderedDict()\n",
    "        self._forward_pre_hooks = OrderedDict()\n",
    "        self._state_dict_hooks = OrderedDict()\n",
    "        self._load_state_dict_pre_hooks = OrderedDict()\n",
    "        self._modules = OrderedDict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 無視するもの\n",
    "- hook系のもの\n",
    "- hookはある関数が実行されたときに補正処理をおこなもののこと\n",
    "  - forward_hooksの場合forward演算で補正がしたい場合の処理一覧が登録\n",
    "  - backward_hooksの場合…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorchの単純なクラスで__init__で定義された値がどうなったかを確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              Parameter containing:\n",
       "              tensor([[ 0.3474,  0.4222, -0.0881],\n",
       "                      [-0.3853, -0.4168, -0.0868],\n",
       "                      [-0.3814, -0.4686,  0.1939],\n",
       "                      [ 0.0700,  0.3962, -0.5166]], requires_grad=True)),\n",
       "             ('bias',\n",
       "              Parameter containing:\n",
       "              tensor([ 0.0721,  0.4652, -0.4289, -0.3902], requires_grad=True))])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = nn.Linear(3, 4)\n",
    "n._parameters\n",
    "# Parameter containing:はParameterクラスの __repr__で加えられたもの"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 後は定義されておらず"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict()\n",
      "OrderedDict()\n",
      "OrderedDict()\n",
      "OrderedDict()\n",
      "OrderedDict()\n",
      "OrderedDict()\n",
      "OrderedDict()\n"
     ]
    }
   ],
   "source": [
    "print(n._buffers)\n",
    "print(n._backward_hooks)\n",
    "print(n._forward_hooks)\n",
    "print(n._forward_pre_hooks)\n",
    "print(n._state_dict_hooks)\n",
    "print(n._load_state_dict_pre_hooks)\n",
    "print(n._modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### よくあるネットワーク定義の場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        print(\"after super modules\", self._modules)\n",
    "        self.l1 = nn.Linear(3, 4)\n",
    "        print(\"after l1 modules\", self._modules)\n",
    "        self.l2 = nn.Linear(4,3)\n",
    "        print(\"after l2 modules\", self._modules)\n",
    "        self.l3 = \"a\"\n",
    "        print(\"after l3 modules\", self._modules)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.l1(x)\n",
    "        return self.l2(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict()\n",
      "OrderedDict()\n",
      "OrderedDict()\n",
      "OrderedDict()\n",
      "OrderedDict()\n",
      "OrderedDict()\n",
      "OrderedDict()\n"
     ]
    }
   ],
   "source": [
    "print(net._parameters)\n",
    "print(net._buffers)\n",
    "print(net._backward_hooks)\n",
    "print(net._forward_hooks)\n",
    "print(net._forward_pre_hooks)\n",
    "print(net._state_dict_hooks)\n",
    "print(net._load_state_dict_pre_hooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('l1', Linear(in_features=3, out_features=4, bias=True)), ('l2', Linear(in_features=4, out_features=3, bias=True))])\n"
     ]
    }
   ],
   "source": [
    "print(net._modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 注意\n",
    "- Netにパラメータがないという意味ではない。\n",
    "  - _parameterはどのクラスで定義されたかを管理\n",
    "  - nn.Modulerのパラメータは parameters()で把握できる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.1978,  0.4489,  0.2995],\n",
      "        [-0.5384, -0.1922, -0.2038],\n",
      "        [ 0.5301,  0.2694, -0.4621],\n",
      "        [ 0.3995, -0.4096,  0.3189]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0071, -0.0110, -0.3840,  0.0931], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.4092,  0.2749, -0.2811, -0.1578],\n",
      "        [ 0.2796,  0.1409, -0.4183, -0.4621],\n",
      "        [-0.4077,  0.3120, -0.0261, -0.3705]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.4393, -0.1938,  0.1679], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for p in net.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### netの作成時で気になるところ\n",
    "- `__init__` しか実行していいはずだが、いつの間にか `._modules`が更新されている"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Moduleの`__setattr__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __setattr__(self, name, value):\n",
    "        \"\"\"\n",
    "           1. valueがParatemerクラスの場合:\n",
    "              register_parameterを実行\n",
    "           2. valueがModuleクラスの場合:\n",
    "              modulesに値を追加\n",
    "           3. それ以外はbufferとして処理\n",
    "        \"\"\"\n",
    "        # 内部用のメソッド定義\n",
    "        def remove_from(*dicts):\n",
    "            # 複数の引数をまとめてlistのように扱う\n",
    "            for d in dicts:\n",
    "                if name in d:\n",
    "                    del d[name]\n",
    "\n",
    "        params = self.__dict__.get('_parameters')\n",
    "        \n",
    "        # valueがParameterの場合\n",
    "        if isinstance(value, Parameter):\n",
    "            if params is None:\n",
    "                raise AttributeError(\n",
    "                    \"cannot assign parameters before Module.__init__() call\")\n",
    "            remove_from(self.__dict__, self._buffers, self._modules)\n",
    "            self.register_parameter(name, value)\n",
    "            \n",
    "        # Parameter名とnameがかぶった場合\n",
    "        # valueがNoneの場合はパラメータを削除する\n",
    "        elif params is not None and name in params:\n",
    "            if value is not None:\n",
    "                raise TypeError(\"cannot assign '{}' as parameter '{}' \"\n",
    "                                \"(torch.nn.Parameter or None expected)\"\n",
    "                                .format(torch.typename(value), name))\n",
    "            self.register_parameter(name, value)\n",
    "        else:\n",
    "            # valueがModuleの場合\n",
    "            modules = self.__dict__.get('_modules')\n",
    "            if isinstance(value, Module):\n",
    "                if modules is None:\n",
    "                    raise AttributeError(\n",
    "                        \"cannot assign module before Module.__init__() call\")\n",
    "                remove_from(self.__dict__, self._parameters, self._buffers)\n",
    "                modules[name] = value\n",
    "            elif modules is not None and name in modules:\n",
    "                if value is not None:\n",
    "                    raise TypeError(\"cannot assign '{}' as child module '{}' \"\n",
    "                                    \"(torch.nn.Module or None expected)\"\n",
    "                                    .format(torch.typename(value), name))\n",
    "                modules[name] = value\n",
    "            else:  \n",
    "                # valueがそれ以外の場合\n",
    "                buffers = self.__dict__.get('_buffers')\n",
    "                if buffers is not None and name in buffers:\n",
    "                    if value is not None and not isinstance(value, torch.Tensor):\n",
    "                        raise TypeError(\"cannot assign '{}' as buffer '{}' \"\n",
    "                                        \"(torch.Tensor or None expected)\"\n",
    "                                        .format(torch.typename(value), name))\n",
    "                    buffers[name] = value\n",
    "                else:\n",
    "                    # bufferでもない場合\n",
    "                    # buffersがNoneでない場合必ずここにこないのは注意\n",
    "                    # bufferでも,moduleでも,parameterでもないクラスは使えない\n",
    "                    object.__setattr__(self, name, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `__setattr__`の書き換え\n",
    "- 長過ぎるので短くする\n",
    "- 3要素に応じて処理を書き換える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __setattr__(self, name, value):\n",
    "    \"\"\"\n",
    "       1. valueがParatemerクラスの場合:\n",
    "          register_parameterを実行\n",
    "       2. valueがModuleクラスの場合:\n",
    "          modulesに値を追加\n",
    "       3. それ以外はbufferとして処理\n",
    "    \"\"\"\n",
    "    # 内部用のメソッド定義\n",
    "    def remove_from(*dicts):\n",
    "        # 複数の引数をまとめてlistのように扱う\n",
    "        for d in dicts:\n",
    "            if name in d:\n",
    "                del d[name]\n",
    "    # 1. \n",
    "    parameter_setted = __parameter__setattr(self, name, value, remove_from)\n",
    "    if not parameter_setted:\n",
    "        # 2. \n",
    "        module_setted = __module_settattr(self, name, value, remove_from)\n",
    "        if not module_setted:\n",
    "            # 3.\n",
    "            __buffer_setattr(self, name, value, remove_from)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. `__parameter__setattr`\n",
    "- valueがパラメータクラスか確認\n",
    "  - パラメータクラスの場合,パラメータを保存する変数が値が追加できる状態か確認し,\n",
    "    parameterに値を追加する\n",
    "  - パラメータクラスでない場合,\n",
    "    parameterに登録された名前と同じnameかつvalueがNoneの時はparmeter削除と判定し、削除処理を実行\n",
    "    それ以外の変な状況はエラーにする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __parameter__setattr(self, name, value, remove_from):\n",
    "    params = self.__dict__.get('_parameters')\n",
    "    if isinstance(value, Parameter):\n",
    "            if params is None:\n",
    "                raise AttributeError(\n",
    "                    \"cannot assign parameters before Module.__init__() call\")\n",
    "            remove_from(self.__dict__, self._buffers, self._modules)\n",
    "            self.register_parameter(name, value)\n",
    "            return True\n",
    "            \n",
    "        # Parameter名とnameがかぶった場合\n",
    "        # valueがNoneの場合はパラメータを削除する\n",
    "    elif params is not None and name in params:\n",
    "        if value is not None:\n",
    "            raise TypeError(\"cannot assign '{}' as parameter '{}' \"\n",
    "                                \"(torch.nn.Parameter or None expected)\"\n",
    "                                .format(torch.typename(value), name))\n",
    "        self.register_parameter(name, value)\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. `__module__setattr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __module__setattr(self, name, value, remove_from):\n",
    "    # valueがModuleの場合\n",
    "    modules = self.__dict__.get('_modules')\n",
    "    if isinstance(value, Module):\n",
    "        if modules is None:\n",
    "            raise AttributeError(\n",
    "                \"cannot assign module before Module.__init__() call\")\n",
    "        remove_from(self.__dict__, self._parameters, self._buffers)\n",
    "        modules[name] = value\n",
    "        return True\n",
    "    elif modules is not None and name in modules:\n",
    "        if value is not None:\n",
    "            raise TypeError(\"cannot assign '{}' as child module '{}' \"\n",
    "                            \"(torch.nn.Module or None expected)\"\n",
    "                            .format(torch.typename(value), name))\n",
    "        modules[name] = value\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. `__buffer__setattr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __buffer__setattr(self, name, value, remove_from):\n",
    "    # valueがそれ以外の場合\n",
    "    buffers = self.__dict__.get('_buffers')\n",
    "    if buffers is not None and name in buffers:\n",
    "        if value is not None and not isinstance(value, torch.Tensor):\n",
    "            raise TypeError(\"cannot assign '{}' as buffer '{}' \"\n",
    "                            \"(torch.Tensor or None expected)\"\n",
    "                            .format(torch.typename(value), name))\n",
    "        buffers[name] = value\n",
    "    else:\n",
    "        # bufferでもない場合\n",
    "        # buffersがNoneでない場合必ずここにこないのは注意\n",
    "        # bufferでも,moduleでも,parameterでもないクラスは使えない\n",
    "        object.__setattr__(self, name, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `__setattr__`に存在する三要素\n",
    "- parameter: NNのパラメータ(実際はnn.Parameterクラスのオブジェクト)\n",
    "- module: nn.Moduleのオブジェクト(Parameterも持つ)\n",
    "- buffer: パラメータにしないTensor\n",
    "- moduleと判定されるかParameterと判定されるかの違い\n",
    "  - 自分のクラスがParameterを持つか\n",
    "  - 自分のattributeがparameterを持つか"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### パラメータの設定詳細\n",
    "- やっている操作\n",
    "  - パラメータクラスかの確認\n",
    "    - `isinstance(object, class)` は、第一引数のオブジェクトが、第二引数の型のインスタンス、またはサブクラスのインスタンスであればTrueを返す関数\n",
    "  - 条件に応じて`register_parameter`を実行しパラメータの追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 普通のモジュールがパラメータかどうかの確認\n",
    "l = nn.Linear(3, 4)\n",
    "isinstance(l, nn.Parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### パラメータの追加"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def register_parameter(self, name, param):\n",
    "    r\"\"\"Adds a parameter to the module.\n",
    "\n",
    "    The parameter can be accessed as an attribute using given name.\n",
    "\n",
    "    Args:\n",
    "        name (string): name of the parameter. The parameter can be accessed\n",
    "            from this module using the given name\n",
    "        param (Parameter): parameter to be added to the module.\n",
    "    \"\"\"\n",
    "    \n",
    "    # パラメーターが登録削除できるかの確認(ここから)\n",
    "    if '_parameters' not in self.__dict__:\n",
    "        raise AttributeError(\n",
    "            \"cannot assign parameter before Module.__init__() call\")\n",
    "\n",
    "    elif not isinstance(name, torch._six.string_classes):\n",
    "        raise TypeError(\"parameter name should be a string. \"\n",
    "                        \"Got {}\".format(torch.typename(name)))\n",
    "    elif '.' in name:\n",
    "        raise KeyError(\"parameter name can't contain \\\".\\\"\")\n",
    "    elif name == '':\n",
    "        raise KeyError(\"parameter name can't be empty string \\\"\\\"\")\n",
    "    elif hasattr(self, name) and name not in self._parameters:\n",
    "        raise KeyError(\"attribute '{}' already exists\".format(name))\n",
    "    # 異常状態の確認(ここから)\n",
    "    \n",
    "    # パラメータの削除\n",
    "    if param is None:\n",
    "        self._parameters[name] = None\n",
    "    \n",
    "    # パラメータが登録できるかどうかの確認\n",
    "    elif not isinstance(param, Parameter):\n",
    "        raise TypeError(\"cannot assign '{}' object to parameter '{}' \"\n",
    "                        \"(torch.nn.Parameter or None required)\"\n",
    "                        .format(torch.typename(param), name))\n",
    "    elif param.grad_fn:\n",
    "        raise ValueError(\n",
    "            \"Cannot assign non-leaf Tensor to parameter '{0}'. Model \"\n",
    "            \"parameters must be created explicitly. To express '{0}' \"\n",
    "            \"as a function of another Tensor, compute the value in \"\n",
    "            \"the forward() method.\".format(name))\n",
    "    else:\n",
    "        # パラメータの登録\n",
    "        self._parameters[name] = param\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `register_parameter`でやっていること\n",
    "- 例外キャッチ多数\n",
    "- 値代入、削除\n",
    "- 実質は以下\n",
    "```python\n",
    "    if param is None:\n",
    "        self._parameters[name] = None\n",
    "    else:\n",
    "        # パラメータの登録\n",
    "        self._parameters[name] = param\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モジュールの設定詳細\n",
    "- Moduleかどうかの判定\n",
    "- Moduleへの代入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 普段扱っているモジュールはnn.Moduleのインスタンス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(n, nn.Module) ## nn.Moduleを継承しているか"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(net, nn.Module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moduleの場合は`_modules`に辞書的に追加\n",
    "```python\n",
    "modules[name] = value\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bufferの代入削除\n",
    "- Moduleと同様"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `nn.Module`のパラメータ確認方法\n",
    "- _parametesは下のクラスで定義されたものは入っていない\n",
    "- ただし、以下等で確認できる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1.weight Parameter containing:\n",
      "tensor([[ 0.1978,  0.4489,  0.2995],\n",
      "        [-0.5384, -0.1922, -0.2038],\n",
      "        [ 0.5301,  0.2694, -0.4621],\n",
      "        [ 0.3995, -0.4096,  0.3189]], requires_grad=True)\n",
      "l1.bias Parameter containing:\n",
      "tensor([ 0.0071, -0.0110, -0.3840,  0.0931], requires_grad=True)\n",
      "l2.weight Parameter containing:\n",
      "tensor([[ 0.4092,  0.2749, -0.2811, -0.1578],\n",
      "        [ 0.2796,  0.1409, -0.4183, -0.4621],\n",
      "        [-0.4077,  0.3120, -0.0261, -0.3705]], requires_grad=True)\n",
      "l2.bias Parameter containing:\n",
      "tensor([-0.4393, -0.1938,  0.1679], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name, parame in net.named_parameters():\n",
    "    print(name, parame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.1978,  0.4489,  0.2995],\n",
      "        [-0.5384, -0.1922, -0.2038],\n",
      "        [ 0.5301,  0.2694, -0.4621],\n",
      "        [ 0.3995, -0.4096,  0.3189]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0071, -0.0110, -0.3840,  0.0931], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.4092,  0.2749, -0.2811, -0.1578],\n",
      "        [ 0.2796,  0.1409, -0.4183, -0.4621],\n",
      "        [-0.4077,  0.3120, -0.0261, -0.3705]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.4393, -0.1938,  0.1679], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for p in net.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('l1.weight',\n",
       "              tensor([[ 0.1978,  0.4489,  0.2995],\n",
       "                      [-0.5384, -0.1922, -0.2038],\n",
       "                      [ 0.5301,  0.2694, -0.4621],\n",
       "                      [ 0.3995, -0.4096,  0.3189]])),\n",
       "             ('l1.bias', tensor([ 0.0071, -0.0110, -0.3840,  0.0931])),\n",
       "             ('l2.weight',\n",
       "              tensor([[ 0.4092,  0.2749, -0.2811, -0.1578],\n",
       "                      [ 0.2796,  0.1409, -0.4183, -0.4621],\n",
       "                      [-0.4077,  0.3120, -0.0261, -0.3705]])),\n",
       "             ('l2.bias', tensor([-0.4393, -0.1938,  0.1679]))])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()\n",
    "# state_dictについては今回はこれ以上紹介しない"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parametersの実装\n",
    "- yieldで必要な情報を加工したgeneratorを作成\n",
    "- 実質必要な情報は_named_membersから取得し、結果を返却"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def parameters(self, recurse=True):\n",
    "        for name, param in self.named_parameters(recurse=recurse):\n",
    "            yield param\n",
    "\n",
    "    def named_parameters(self, prefix='', recurse=True):\n",
    "        gen = self._named_members(\n",
    "            # モジュールに対し、そのモジュールの持つパラメータ一覧を取得\n",
    "            lambda module: module._parameters.items(),\n",
    "            prefix=prefix, recurse=recurse)\n",
    "        # モジュールごとにモジュールの持つパラメータ一覧を返却する\n",
    "        for elem in gen:\n",
    "            yield elem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モジュールの一覧からget_members_fnで必要な情報一覧を取得する\n",
    "# どこから取得したかわかるようにmodule_prefix等で補正する\n",
    "def _named_members(self, get_members_fn, prefix='', recurse=True):\n",
    "    r\"\"\"Helper method for yielding various names + members of modules.\"\"\"\n",
    "    memo = set()\n",
    "    modules = self.named_modules(prefix=prefix) if recurse else [(prefix, self)]\n",
    "    for module_prefix, module in modules:\n",
    "        members = get_members_fn(module)\n",
    "        for k, v in members:\n",
    "            if v is None or v in memo:\n",
    "                continue\n",
    "            memo.add(v)\n",
    "            name = module_prefix + ('.' if module_prefix else '') + k\n",
    "            yield name, v\n",
    "\n",
    "# モジュール一覧の取得\n",
    "def named_modules(self, memo=None, prefix=''):\n",
    "    if memo is None:\n",
    "        memo = set()\n",
    "    if self not in memo:\n",
    "        # 自分自信を追加\n",
    "        memo.add(self)\n",
    "        yield prefix, self\n",
    "        # 自分のモジュール一覧を取得\n",
    "        for name, module in self._modules.items():\n",
    "            if module is None:\n",
    "                continue\n",
    "            # モジュールのprefixを追加\n",
    "            submodule_prefix = prefix + ('.' if prefix else '') + name\n",
    "            for m in module.named_modules(memo, submodule_prefix):\n",
    "                yield m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 関係を改めてまとめる\n",
    "- parameters():\n",
    "  - パラメータを返すジェネレーター\n",
    "  - 実装上はnamed_parameters（）から名前を捨てただけ\n",
    "- named_parametes():\n",
    "  - パラメータの名前と値を返すジェネレータ\n",
    "  - 実際はモジュール一覧から` lambda module: module._parameters.items(),`で処理をした情報を受けとっただけ\n",
    "- _named_members():\n",
    "  - モジュールの名前と`get_members_fn`で選択した情報を返すジェネレータ\n",
    "  - モジュール一覧は`named_modules`から取得\n",
    "- named_modules():\n",
    "  - モジュール一覧の取得\n",
    "  - モジュールの名前でネストした名前をとモジュールの組を返す"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### クラスの実行\n",
    "- 処理は\n",
    "  - hookに基づき入力を変更\n",
    "  - forwardの実行\n",
    "  - forward結果に対するhookの実行\n",
    "  - grad_fnの登録(計算グラフのデバッグ用)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __call__(self, *input, **kwargs):\n",
    "        for hook in self._forward_pre_hooks.values():\n",
    "            result = hook(self, input)\n",
    "            if result is not None:\n",
    "                if not isinstance(result, tuple):\n",
    "                    result = (result,)\n",
    "                input = result\n",
    "        if torch._C._get_tracing_state():\n",
    "            result = self._slow_forward(*input, **kwargs)\n",
    "        else:\n",
    "            result = self.forward(*input, **kwargs)\n",
    "        for hook in self._forward_hooks.values():\n",
    "            hook_result = hook(self, input, result)\n",
    "            if hook_result is not None:\n",
    "                result = hook_result\n",
    "        if len(self._backward_hooks) > 0:\n",
    "            var = result\n",
    "            while not isinstance(var, torch.Tensor):\n",
    "                if isinstance(var, dict):\n",
    "                    var = next((v for v in var.values() if isinstance(v, torch.Tensor)))\n",
    "                else:\n",
    "                    var = var[0]\n",
    "            grad_fn = var.grad_fn\n",
    "            if grad_fn is not None:\n",
    "                for hook in self._backward_hooks.values():\n",
    "                    wrapper = functools.partial(hook, self)\n",
    "                    functools.update_wrapper(wrapper, hook)\n",
    "                    grad_fn.register_hook(wrapper)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## backwardの計算\n",
    "- torch.autograd.backwardで行う\n",
    "- 詳細はCレイヤなので省略\n",
    "- 内部的にはfunctionが前のfunctionを覚えていて、それでどうにかする形\n",
    "- `loss.grad_fn.next_functions`をすると一つ前の関数がわかる\n",
    "\n",
    "### 重要なポイント\n",
    "- 計算グラフ情報は`nn.Module`が覚えているわけではない\n",
    "- これによって`nn.Module`を複数回使うことも容易になる(RNN等)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `nn.Module`まとめ\n",
    "- parameterやmoduleをbufferを設定できる\n",
    "- moduleによる再帰構造が可能\n",
    "- forwardを定義すればそれに基づき計算する\n",
    "- backwardはnn.Module内には存在しない"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Moduleの典型例\n",
    "- nn.Sequnetial\n",
    "- nn.ModuleList\n",
    "- nn.Liner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Sequential\n",
    "- Sequentialクラスにmoduleを加える\n",
    "- forwardでそれを実行する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch._jit_internal import _copy_to_script_wrapper\n",
    "\n",
    "class Sequential(nn.Module):\n",
    "    r\"\"\"A sequential container.\n",
    "    Modules will be added to it in the order they are passed in the constructor.\n",
    "    Alternatively, an ordered dict of modules can also be passed in.\n",
    "\n",
    "    To make it easier to understand, here is a small example::\n",
    "\n",
    "        # Example of using Sequential\n",
    "        model = nn.Sequential(\n",
    "                  nn.Conv2d(1,20,5),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Conv2d(20,64,5),\n",
    "                  nn.ReLU()\n",
    "                )\n",
    "\n",
    "        # Example of using Sequential with OrderedDict\n",
    "        model = nn.Sequential(OrderedDict([\n",
    "                  ('conv1', nn.Conv2d(1,20,5)),\n",
    "                  ('relu1', nn.ReLU()),\n",
    "                  ('conv2', nn.Conv2d(20,64,5)),\n",
    "                  ('relu2', nn.ReLU())\n",
    "                ]))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args):\n",
    "        super(Sequential, self).__init__()\n",
    "        if len(args) == 1 and isinstance(args[0], OrderedDict):\n",
    "            for key, module in args[0].items():\n",
    "                self.add_module(key, module)\n",
    "        else:\n",
    "            for idx, module in enumerate(args):\n",
    "                self.add_module(str(idx), module)\n",
    "\n",
    "    def _get_item_by_idx(self, iterator, idx):\n",
    "        \"\"\"Get the idx-th item of the iterator\"\"\"\n",
    "        size = len(self)\n",
    "        idx = operator.index(idx)\n",
    "        if not -size <= idx < size:\n",
    "            raise IndexError('index {} is out of range'.format(idx))\n",
    "        idx %= size\n",
    "        return next(islice(iterator, idx, None))\n",
    "\n",
    "    @_copy_to_script_wrapper\n",
    "    # itemを取得\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, slice):\n",
    "            return self.__class__(OrderedDict(list(self._modules.items())[idx]))\n",
    "        else:\n",
    "            return self._get_item_by_idx(self._modules.values(), idx)\n",
    "\n",
    "    # itemを変更\n",
    "    def __setitem__(self, idx, module):\n",
    "        key = self._get_item_by_idx(self._modules.keys(), idx)\n",
    "        return setattr(self, key, module)\n",
    "    \n",
    "    # itemを削除\n",
    "    def __delitem__(self, idx):\n",
    "        if isinstance(idx, slice):\n",
    "            for key in list(self._modules.keys())[idx]:\n",
    "                delattr(self, key)\n",
    "        else:\n",
    "            key = self._get_item_by_idx(self._modules.keys(), idx)\n",
    "            delattr(self, key)\n",
    "\n",
    "    @_copy_to_script_wrapper\n",
    "    # 長さを取得\n",
    "    def __len__(self):\n",
    "        return len(self._modules)\n",
    "\n",
    "    @_copy_to_script_wrapper\n",
    "    def __dir__(self):\n",
    "        keys = super(Sequential, self).__dir__()\n",
    "        keys = [key for key in keys if not key.isdigit()]\n",
    "        return keys\n",
    "\n",
    "    @_copy_to_script_wrapper\n",
    "    def __iter__(self):\n",
    "        return iter(self._modules.values())\n",
    "\n",
    "    def forward(self, input):\n",
    "        for module in self:\n",
    "            input = module(input)\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = Sequential(nn.Linear(2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=2, out_features=3, bias=True)\n"
     ]
    }
   ],
   "source": [
    "for mod in seq:\n",
    "    print(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq.add_module(\"l2\", nn.Linear(3,5))\n",
    "# operator等が定義されていないので動作しないものもある"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2, out_features=3, bias=True)\n",
       "  (l2): Linear(in_features=3, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seqeuntialのポイント\n",
    "- forwardが登録した順になる\n",
    "- ネットワークが分岐するような特殊なものは作れない"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.ModuleList\n",
    "- moduleのListを管理\n",
    "- forwardはサポートせず、自分で各moduleごとに実行する\n",
    "\n",
    "### 特徴\n",
    "- iadd (+=)が実装\n",
    "- appendが実装\n",
    "- extendが実装\n",
    "- forwardが実装されず"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModuleList(nn.Module):\n",
    "    r\"\"\"Holds submodules in a list.\n",
    "\n",
    "    :class:`~torch.nn.ModuleList` can be indexed like a regular Python list, but\n",
    "    modules it contains are properly registered, and will be visible by all\n",
    "    :class:`~torch.nn.Module` methods.\n",
    "\n",
    "    Arguments:\n",
    "        modules (iterable, optional): an iterable of modules to add\n",
    "\n",
    "    Example::\n",
    "\n",
    "        class MyModule(nn.Module):\n",
    "            def __init__(self):\n",
    "                super(MyModule, self).__init__()\n",
    "                self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)])\n",
    "\n",
    "            def forward(self, x):\n",
    "                # ModuleList can act as an iterable, or be indexed using ints\n",
    "                for i, l in enumerate(self.linears):\n",
    "                    x = self.linears[i // 2](x) + l(x)\n",
    "                return x\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, modules=None):\n",
    "        super(ModuleList, self).__init__()\n",
    "        if modules is not None:\n",
    "            self += modules\n",
    "\n",
    "    def _get_abs_string_index(self, idx):\n",
    "        \"\"\"Get the absolute index for the list of modules\"\"\"\n",
    "        idx = operator.index(idx)\n",
    "        if not (-len(self) <= idx < len(self)):\n",
    "            raise IndexError('index {} is out of range'.format(idx))\n",
    "        if idx < 0:\n",
    "            idx += len(self)\n",
    "        return str(idx)\n",
    "\n",
    "    @_copy_to_script_wrapper\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, slice):\n",
    "            return self.__class__(list(self._modules.values())[idx])\n",
    "        else:\n",
    "            return self._modules[self._get_abs_string_index(idx)]\n",
    "\n",
    "    def __setitem__(self, idx, module):\n",
    "        idx = self._get_abs_string_index(idx)\n",
    "        return setattr(self, str(idx), module)\n",
    "\n",
    "    def __delitem__(self, idx):\n",
    "        if isinstance(idx, slice):\n",
    "            for k in range(len(self._modules))[idx]:\n",
    "                delattr(self, str(k))\n",
    "        else:\n",
    "            delattr(self, self._get_abs_string_index(idx))\n",
    "        # To preserve numbering, self._modules is being reconstructed with modules after deletion\n",
    "        str_indices = [str(i) for i in range(len(self._modules))]\n",
    "        self._modules = OrderedDict(list(zip(str_indices, self._modules.values())))\n",
    "\n",
    "    @_copy_to_script_wrapper\n",
    "    def __len__(self):\n",
    "        return len(self._modules)\n",
    "\n",
    "    @_copy_to_script_wrapper\n",
    "    def __iter__(self):\n",
    "        return iter(self._modules.values())\n",
    "\n",
    "    def __iadd__(self, modules):\n",
    "        return self.extend(modules)\n",
    "\n",
    "    @_copy_to_script_wrapper\n",
    "    def __dir__(self):\n",
    "        keys = super(ModuleList, self).__dir__()\n",
    "        keys = [key for key in keys if not key.isdigit()]\n",
    "        return keys\n",
    "\n",
    "    def insert(self, index, module):\n",
    "        r\"\"\"Insert a given module before a given index in the list.\n",
    "\n",
    "        Arguments:\n",
    "            index (int): index to insert.\n",
    "            module (nn.Module): module to insert\n",
    "        \"\"\"\n",
    "        for i in range(len(self._modules), index, -1):\n",
    "            self._modules[str(i)] = self._modules[str(i - 1)]\n",
    "        self._modules[str(index)] = module\n",
    "\n",
    "    def append(self, module):\n",
    "        r\"\"\"Appends a given module to the end of the list.\n",
    "\n",
    "        Arguments:\n",
    "            module (nn.Module): module to append\n",
    "        \"\"\"\n",
    "        self.add_module(str(len(self)), module)\n",
    "        return self\n",
    "\n",
    "    def extend(self, modules):\n",
    "        r\"\"\"Appends modules from a Python iterable to the end of the list.\n",
    "\n",
    "        Arguments:\n",
    "            modules (iterable): iterable of modules to append\n",
    "        \"\"\"\n",
    "        # container_abcs.Iterableがここでは定義されてないためエラーになる\n",
    "        if not isinstance(modules, container_abcs.Iterable):\n",
    "            raise TypeError(\"ModuleList.extend should be called with an \"\n",
    "                            \"iterable, but got \" + type(modules).__name__)\n",
    "        offset = len(self)\n",
    "        for i, module in enumerate(modules):\n",
    "            self.add_module(str(offset + i), module)\n",
    "        return self\n",
    "\n",
    "    def forward(self):\n",
    "        raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (1): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (2): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (3): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (4): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (5): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (6): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (7): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (8): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (9): Linear(in_features=10, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.ModuleList([nn.Linear(10, 10) for i in range(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Linear\n",
    "- weightをパラメータに\n",
    "- biasをパラメータに\n",
    "- weightとbiasの初期値を設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    __constants__ = ['in_features', 'out_features']\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(Linear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    # パラメータの初期値を設定\n",
    "    def reset_parameters(self):\n",
    "        # kaiming_uniformと呼ばれる一様分布でパラメータを初期化\n",
    "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # nn.functionalと呼ばれる演算の定義されたクラス\n",
    "        # 内部は実際はCを呼ぶ\n",
    "        return F.linear(input, self.weight, self.bias)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return 'in_features={}, out_features={}, bias={}'.format(\n",
    "            self.in_features, self.out_features, self.bias is not None\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss関数を定義する\n",
    "- 損失はbufferに登録\n",
    "- 実際の計算は`F.cross_entropy`:\n",
    "  - これもC側の実装\n",
    "  - weightは特定の項目の重み付けを変更するもの"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Loss(Module):\n",
    "    def __init__(self, size_average=None, reduce=None, reduction='mean'):\n",
    "        super(_Loss, self).__init__()\n",
    "        if size_average is not None or reduce is not None:\n",
    "            self.reduction = _Reduction.legacy_get_string(size_average, reduce)\n",
    "        else:\n",
    "            self.reduction = reduction\n",
    "\n",
    "class _WeightedLoss(_Loss):\n",
    "    def __init__(self, weight=None, size_average=None, reduce=None, reduction='mean'):\n",
    "        super(_WeightedLoss, self).__init__(size_average, reduce, reduction)\n",
    "        # 損失はparameterではないのでbufferに登録\n",
    "        self.register_buffer('weight', weight)\n",
    "\n",
    "\n",
    "class CrossEntropyLoss(_WeightedLoss):\n",
    "    def __init__(self, weight=None, size_average=None, ignore_index=-100,\n",
    "                 reduce=None, reduction='mean'):\n",
    "        super(CrossEntropyLoss, self).__init__(weight, size_average, reduce, reduction)\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        return F.cross_entropy(input, target, weight=self.weight,\n",
    "                               ignore_index=self.ignore_index, reduction=self.reduction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "\n",
    "class SGD(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=required, momentum=0, dampening=0,\n",
    "                 weight_decay=0, nesterov=False):\n",
    "        if lr is not required and lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if momentum < 0.0:\n",
    "            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n",
    "        if weight_decay < 0.0:\n",
    "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
    "\n",
    "        defaults = dict(lr=lr, momentum=momentum, dampening=dampening,\n",
    "                        weight_decay=weight_decay, nesterov=nesterov)\n",
    "        if nesterov and (momentum <= 0 or dampening != 0):\n",
    "            raise ValueError(\"Nesterov momentum requires a momentum and zero dampening\")\n",
    "        super(SGD, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(SGD, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('nesterov', False)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            weight_decay = group['weight_decay']\n",
    "            momentum = group['momentum']\n",
    "            dampening = group['dampening']\n",
    "            nesterov = group['nesterov']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                d_p = p.grad\n",
    "                if weight_decay != 0:\n",
    "                    d_p = d_p.add(p, alpha=weight_decay)\n",
    "                if momentum != 0:\n",
    "                    param_state = self.state[p]\n",
    "                    if 'momentum_buffer' not in param_state:\n",
    "                        buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                    else:\n",
    "                        buf = param_state['momentum_buffer']\n",
    "                        buf.mul_(momentum).add_(d_p, alpha=1 - dampening)\n",
    "                    if nesterov:\n",
    "                        d_p = d_p.add(buf, alpha=momentum)\n",
    "                    else:\n",
    "                        d_p = buf\n",
    "\n",
    "                p.add_(d_p, alpha=-group['lr'])\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `__init__`\n",
    "- パラメータの妥当性チェック\n",
    "- 正常な時のパラメータ設定\n",
    "```python\n",
    "        defaults = dict(lr=lr, momentum=momentum, dampening=dampening,\n",
    "                        weight_decay=weight_decay, nesterov=nesterov)\n",
    "        super(SGD, self).__init__(params, defaults)\n",
    "```\n",
    "\n",
    "- self.param_groups:\n",
    "  - groupの配列\n",
    "  - groupはParameterクラスと`defaults`を加えた`dict`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `step`\n",
    "- 実質以下\n",
    "  - 実際はmomentum,dampening,nesterov等SGDの亜種にするための設定を加える\n",
    "  - weight_decay等のパラメータの設定も加える\n",
    "```python\n",
    "for p in group['params']:\n",
    "    d_p = p.grad\n",
    "    p.add_(d_p, alpha=-group['lr'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 改めて\n",
    "\n",
    "- `net = nn.Module()` ニューラルネットワーク自体の定義\n",
    "- `loss = criterion(net(x), y)` 損失の定義\n",
    "- `loss.backward()`: 損失によるパラメータの微分\n",
    "- `optimzer.step()`: optimizerに設定したパラメータの更新\n",
    "\n",
    "後はこれをデータとEpoch数だけ繰り返せば良い"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6504, 0.0000, 0.0000, 0.2218], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, networks):\n",
    "        super(Net, self).__init__()\n",
    "        self.networks = networks\n",
    "\n",
    "    def forward(self, x):\n",
    "        for net in self.networks:\n",
    "            x = net(x)\n",
    "            x = nn.ReLU()(x)\n",
    "        return x\n",
    "net = Net([nn.Linear(3,5), nn.Linear(5, 4)])\n",
    "net(torch.tensor([1., 2., 3.]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 演習\n",
    "以下のソースコードはそのままでは動かない.何箇所か修正し,活性化関数ReLUの3層のnnの計算を実行せよ.\n",
    "```python\n",
    "class Net(Module):\n",
    "    def __init__(self, netwoerks):\n",
    "        self.networks = networks\n",
    "\n",
    "    def forward(self, x):\n",
    "        for net in self.networks:\n",
    "            net(x)\n",
    "net = Net([nn.Linear(3,5), nn.Linear(5, 4)])\n",
    "net(torch.tensor([1, 2, 3]))\n",
    "```\n",
    "\n",
    "## 誤り\n",
    "- Moduleクラスが定義されていない(かもしれない)\n",
    "- `super`が実行されていない\n",
    "- 引数がnetwoerksになっている\n",
    "- forwardでnetの結果が反映されていない\n",
    "- 活性化関数が定義されていない\n",
    "- forwardの結果がReturnされていない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1999, 0.0000, 0.3270, 0.0000], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, networks):\n",
    "        super(Net, self).__init__()\n",
    "        self.networks = networks\n",
    "\n",
    "    def forward(self, x):\n",
    "        for net in self.networks:\n",
    "            x = net(x)\n",
    "            x = nn.ReLU()(x)\n",
    "        return x\n",
    "net = Net([nn.Linear(3,5), nn.Linear(5, 4)])\n",
    "net(torch.tensor([1., 2., 3.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "optimizer got an empty parameter list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-145caa95c8fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/pytorch-sample-IYnN251Q-py3.7/lib/python3.7/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, momentum, dampening, weight_decay, nesterov)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnesterov\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmomentum\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdampening\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Nesterov momentum requires a momentum and zero dampening\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/pytorch-sample-IYnN251Q-py3.7/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mparam_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"optimizer got an empty parameter list\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mparam_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparam_groups\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: optimizer got an empty parameter list"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        l1 = nn.Linear(4, 3)\n",
    "        return l1(x)\n",
    "    \n",
    "net, x, y = Net(), torch.tensor([[1,2,3,4]]), torch.tensor([1])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(),lr=0.01)\n",
    "loss = net(x)\n",
    "[loss.backward() for _ in range(100)]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = nn.Linear(4, 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.l1(x)\n",
    "    \n",
    "net, x, y = Net(), torch.tensor([[1,2,3,4]]).float(), torch.tensor([1])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "for _ in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(net(x), y)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.1160,  2.6083, -1.6311]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 誤り"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tensor() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-5ad8091b430f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: tensor() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.nn_list = [nn.Linear(4,4), nn.Linear(4, 3)]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for l in nn_list:\n",
    "            x = nn.ReLU()(l(x))\n",
    "        return x,\n",
    "net, x, y = Net(), torch.tensor([1,2,3,4], [2,3,10, 1]), torch.tensor([1, 2])\n",
    "criterion = nn.MSe()\n",
    "optimizer = torch.optim.SGD(net.parameters())\n",
    "for i in range(100):\n",
    "    criterion(net(x), y).backward(); optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訂正"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5021],\n",
      "        [1.2318]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.0000],\n",
      "        [1.7440]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.0000],\n",
      "        [1.7440]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.0000],\n",
      "        [1.7440]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.0000],\n",
      "        [1.7440]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.0000],\n",
      "        [1.7440]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.0000],\n",
      "        [1.7440]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.0000],\n",
      "        [1.7440]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.0000],\n",
      "        [1.7440]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.0000],\n",
      "        [1.7440]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.nn_list = nn.ModuleList([nn.Linear(4,4), nn.Linear(4, 1)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for l in self.nn_list:\n",
    "            x = nn.ReLU()(l(x))\n",
    "        return x\n",
    "net, x, y = Net(), torch.tensor([[1,2,3,4], [2,3,10, 1]]).float(), torch.tensor([1, 2]).float()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "for i in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    criterion(net(x)[0], y.view(-1, 1)[0]).backward()\n",
    "    if i % 100 == 0:\n",
    "        print(net(x))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000],\n",
       "        [1.7440]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  2.,  3.,  4.],\n",
       "        [ 2.,  3., 10.,  1.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[1,2,3,4], [2,3,10, 1]]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2.])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [2.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.7521],\n",
       "        [1.7521]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'builtin_function_or_method' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-848062825959>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'builtin_function_or_method' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(3,5)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(5,4)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.l2(self.relu(self.l1(torch.tensor[x])))\n",
    "\n",
    "net = Net()\n",
    "net.forward(torch.tensor[1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## デバッグ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.nn_list = [nn.Linear(4,4), nn.Linear(4, 3)]\n",
    "        self.layer = nn.ModuleList(self.nn_list)\n",
    "    def forward(self, x):\n",
    "        for l in self.layer:\n",
    "            x = F.relu(l(x))\n",
    "        return x\n",
    "\n",
    "net, x, y = Net(), torch.tensor([1.0,2.0,3.0,4.0]), torch.tensor([1])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(),0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 2.5897, 0.0000], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1, grad_fn=<NotImplemented>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs=net(x)\n",
    "    criterion(outputs.view(1,-1), y).backward(); \n",
    "    optimizer.step()\n",
    "print(torch.argmax(net(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=4, out_features=3, bias=True)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.ModuleList([nn.Linear(4,4), nn.Linear(4, 3)])[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2091, 0.0622, 0.0218, 0.4440], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Net, self).__init__()\n",
    "    \n",
    "  def forward(self, x):\n",
    "    for net in self.networks:\n",
    "      x = net(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "net.networks = ([nn.Linear(3, 5), nn.ReLU(), nn.Linear(5, 4), nn.ReLU()])\n",
    "net(torch.tensor([1., 2., 3.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 演習1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下を実装\n",
    "- AND\n",
    "- OR\n",
    "- NAND\n",
    "- XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AND(x, y):\n",
    "    w_1 = 0.5\n",
    "    w_2 = 0.5\n",
    "    theta = 0.7\n",
    "    return 1 if  w_1 * x + w_2 * y >= theta else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0 1\n"
     ]
    }
   ],
   "source": [
    "print(AND(1, 0), AND(0, 1), AND(0, 0), AND(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OR(x, y):\n",
    "    w_1 = 0.5\n",
    "    w_2 = 0.5\n",
    "    theta = 0.3\n",
    "    return 1 if  w_1 * x + w_2 * y >= theta else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 0 1\n"
     ]
    }
   ],
   "source": [
    "print(OR(1, 0), OR(0, 1), OR(0, 0), OR(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NAND(x, y):\n",
    "    w_1 = - 0.5\n",
    "    w_2 = - 0.5\n",
    "    theta = - 0.7\n",
    "    return 1 if  w_1 * x + w_2 * y >= theta else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 1 0\n"
     ]
    }
   ],
   "source": [
    "print(NAND(1, 0), NAND(0, 1), NAND(0, 0), NAND(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XOR(x, y):\n",
    "    return AND(NAND(x, y), OR(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 0 0\n"
     ]
    }
   ],
   "source": [
    "print(XOR(1, 0), XOR(0, 1), XOR(0, 0), XOR(1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "演習\n",
    "- 3層のNNを実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = nn.Linear(3, 5)\n",
    "        self.l2 = nn.Linear(5, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.l1(x)\n",
    "        y = self.l2(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0873, -0.3053, -0.1162, -0.0916], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "x = torch.arange(3).float()\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 演習\n",
    "IRISをSGDで学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import namedtuple\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = nn.Linear(4, 10)\n",
    "        self.l2 = nn.Linear(10, 8)\n",
    "        self.l3 = nn.Linear(8, 3)\n",
    "    \n",
    "    def forward(self ,x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        return self.l3(x)\n",
    "\n",
    "def train_epoch(model, data_loader, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(data_loader): # 入力と正解\n",
    "         optimizer.zero_grad() # Weightの初期化\n",
    "         output = model(data) # 仮説で値代入\n",
    "         output.dtype\n",
    "         loss = criterion(output, target) # 損失\n",
    "         loss.backward() # 微分の計算\n",
    "         optimizer.step() # パラメータの更新\n",
    "         \n",
    "    if epoch % 10 == 0:\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            batch_idx, batch_idx * len(data), len(data_loader.dataset),\n",
    "            100. * batch_idx / len(data_loader), loss.item()))\n",
    "\n",
    "def valid_epoch(model, data_loader, epoch):\n",
    "    model.valid()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(data_loader): # 入力と正解\n",
    "             optimizer.zero_grad() # Weightの初期化\n",
    "             output = model(data) # 仮説で値代入\n",
    "             output.dtype\n",
    "             loss = criterion(output, target) # 損失\n",
    "            \n",
    "        if epoch % 10 == 0:\n",
    "             print('Test Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                 batch_idx, batch_idx * len(data), len(data_loader.dataset),\n",
    "                 100. * batch_idx / len(data_loader), loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 39 [117/120 (98%)]\tLoss: 1.284956\n",
      "Test Epoch: 9 [27/30 (90%)]\tLoss: 1.007852\n",
      "Train Epoch: 39 [117/120 (98%)]\tLoss: 0.787336\n",
      "Test Epoch: 9 [27/30 (90%)]\tLoss: 0.915068\n",
      "Train Epoch: 39 [117/120 (98%)]\tLoss: 0.096359\n",
      "Test Epoch: 9 [27/30 (90%)]\tLoss: 0.302962\n",
      "Train Epoch: 39 [117/120 (98%)]\tLoss: 0.295645\n",
      "Test Epoch: 9 [27/30 (90%)]\tLoss: 0.181136\n",
      "Train Epoch: 39 [117/120 (98%)]\tLoss: 0.605164\n",
      "Test Epoch: 9 [27/30 (90%)]\tLoss: 0.241318\n",
      "Train Epoch: 39 [117/120 (98%)]\tLoss: 0.084802\n",
      "Test Epoch: 9 [27/30 (90%)]\tLoss: 0.050701\n",
      "Train Epoch: 39 [117/120 (98%)]\tLoss: 0.051883\n",
      "Test Epoch: 9 [27/30 (90%)]\tLoss: 0.024192\n",
      "Train Epoch: 39 [117/120 (98%)]\tLoss: 0.037929\n",
      "Test Epoch: 9 [27/30 (90%)]\tLoss: 0.317649\n",
      "Train Epoch: 39 [117/120 (98%)]\tLoss: 0.069343\n",
      "Test Epoch: 9 [27/30 (90%)]\tLoss: 0.136748\n",
      "Train Epoch: 39 [117/120 (98%)]\tLoss: 0.003237\n",
      "Test Epoch: 9 [27/30 (90%)]\tLoss: 0.010801\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(iris.data, iris.target, test_size=0.2)\n",
    "\n",
    "X_train = torch.tensor(X_train).float()\n",
    "y_train = torch.tensor(y_train)\n",
    "X_valid = torch.tensor(X_valid).float()\n",
    "y_valid = torch.tensor(y_valid)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "valid_dataset = torch.utils.data.TensorDataset(X_valid, y_valid)\n",
    "\n",
    "model = Net()\n",
    "\n",
    "batch_size  = 3 # ミニバッチのデータの数\n",
    "max_epoch = 100 #\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                   batch_size=batch_size, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, \n",
    "                   batch_size=batch_size, shuffle=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # 損失の定義\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01) #(確率的)勾配降下法\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    train_epoch(model, train_loader, epoch)\n",
    "    valid_epoch(model, valid_loader, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict()"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._backward_hooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 演習:計算グラフに従って微分の計算をする\n",
    "- 計算グラフは書いていながら、NNのBPは計算グラフに沿っているので、参考までに"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([2.0 ,3.0],  requires_grad=True)\n",
    "W = torch.tensor([[0., 1.], [2., 3.]], requires_grad=True)\n",
    "y = torch.tensor([0., 1.], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = (y - W * x)** 2\n",
    "loss = loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -8., -20.])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([16., 52.])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0., 12.],\n",
       "        [16., 48.]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PytorchでのOptimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = nn.Linear(3, 5)\n",
    "        self.l2 = nn.Linear(5, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.l1(x)\n",
    "        y = self.l2(y)\n",
    "        return y\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adadelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "adadelta = torch.optim.Adadelta(net.parameters(), lr=1.0, rho=0.9, eps=1e-06, weight_decay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.optimizer import Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adadelta(Optimizer):\n",
    "    \"\"\"Implements Adadelta algorithm.\n",
    "\n",
    "    It has been proposed in `ADADELTA: An Adaptive Learning Rate Method`__.\n",
    "\n",
    "    Arguments:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        rho (float, optional): coefficient used for computing a running average\n",
    "            of squared gradients (default: 0.9)\n",
    "        eps (float, optional): term added to the denominator to improve\n",
    "            numerical stability (default: 1e-6)\n",
    "        lr (float, optional): coefficient that scale delta before it is applied\n",
    "            to the parameters (default: 1.0)\n",
    "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
    "\n",
    "    __ https://arxiv.org/abs/1212.5701\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=1.0, rho=0.9, eps=1e-6, weight_decay=0):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= rho <= 1.0:\n",
    "            raise ValueError(\"Invalid rho value: {}\".format(rho))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= weight_decay:\n",
    "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
    "\n",
    "        defaults = dict(lr=lr, rho=rho, eps=eps, weight_decay=weight_decay)\n",
    "        super(Adadelta, self).__init__(params, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('Adadelta does not support sparse gradients')\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['square_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    state['acc_delta'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "\n",
    "                square_avg, acc_delta = state['square_avg'], state['acc_delta']\n",
    "                rho, eps = group['rho'], group['eps']\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    grad = grad.add(p, alpha=group['weight_decay'])\n",
    "\n",
    "                square_avg.mul_(rho).addcmul_(grad, grad, value=1 - rho)\n",
    "                std = square_avg.add(eps).sqrt_()\n",
    "                delta = acc_delta.add(eps).sqrt_().div_(std).mul_(grad)\n",
    "                p.add_(delta, alpha=-group['lr'])\n",
    "                acc_delta.mul_(rho).addcmul_(delta, delta, value=1 - rho)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'params': [Parameter containing:\n",
       "   tensor([[-0.0827, -0.2545,  0.0653],\n",
       "           [ 0.1403,  0.0586, -0.4727],\n",
       "           [ 0.2888, -0.5296, -0.0437],\n",
       "           [ 0.1436,  0.3394,  0.5137],\n",
       "           [-0.4268, -0.0852,  0.1617]], requires_grad=True),\n",
       "   Parameter containing:\n",
       "   tensor([ 0.5146,  0.3544, -0.5008,  0.4203,  0.0027], requires_grad=True),\n",
       "   Parameter containing:\n",
       "   tensor([[-0.2886,  0.2312,  0.2758, -0.3287,  0.2043],\n",
       "           [ 0.0142,  0.1444,  0.0924,  0.2030, -0.1304],\n",
       "           [-0.2577, -0.1107, -0.1134,  0.2369,  0.4067],\n",
       "           [ 0.3083,  0.1643,  0.2341,  0.3252,  0.1663]], requires_grad=True),\n",
       "   Parameter containing:\n",
       "   tensor([-0.3942, -0.1765,  0.2970,  0.2063], requires_grad=True)],\n",
       "  'lr': 1.0,\n",
       "  'rho': 0.9,\n",
       "  'eps': 1e-06,\n",
       "  'weight_decay': 0}]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adadelta.param_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
